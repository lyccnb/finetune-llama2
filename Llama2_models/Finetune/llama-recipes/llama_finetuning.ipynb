{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/users/swang299/code/AntGPT-Llama2/llama-recipes\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "\n",
    "# Unused imports removed\n",
    "from utils import fsdp_auto_wrap_policy\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch.distributed as dist\n",
    "# Unused imports removed\n",
    "from utils.train_utils import (\n",
    "    set_tokenizer_params,\n",
    "    train,\n",
    "    evaluation,\n",
    "    freeze_transformer_layers,\n",
    "    check_frozen_layers_peft_model,\n",
    "    setup,\n",
    "    setup_environ_flags,\n",
    "    cleanup,\n",
    "    clear_gpu_cache,\n",
    "    get_parameter_dtypes,\n",
    "    print_model_size,\n",
    "    get_policies  \n",
    ")\n",
    "\n",
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "\n",
    "from utils.config_utils import (\n",
    "    update_config,\n",
    "    generate_peft_config,\n",
    "    generate_dataset_config,\n",
    ")\n",
    "from peft import get_peft_model, TaskType, prepare_model_for_int8_training\n",
    "import configs\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    ")\n",
    "from torch.utils.data import DistributedSampler\n",
    "import policies\n",
    "from policies import AnyPrecisionAdamW\n",
    "from configs import fsdp_config, train_config\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pkg_resources import packaging\n",
    "import torch\n",
    "import torch.cuda.nccl as nccl\n",
    "import torch.distributed as dist\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "\n",
    "# Update the configuration for the training and sharding process\n",
    "kwags = {\"use_peft\":True, \"peft_method\":\"lora\", \"quantization\":True, \"model_name\":'/gpfs/data/superlab/models/llama2/llama/checkpoints/hf/Llama-2-7b-hf', \"output_dir\":\"./\", \"dataset\":\"ego4d_lta_dataset\"}\n",
    "update_config((train_config, fsdp_config), **kwags)\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "torch.cuda.manual_seed(train_config.seed)\n",
    "torch.manual_seed(train_config.seed)\n",
    "\n",
    "if train_config.enable_fsdp:\n",
    "    setup()\n",
    "    # torchrun specific\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "if torch.distributed.is_initialized():\n",
    "    torch.cuda.set_device(rank)\n",
    "    setup_environ_flags(rank)\n",
    "\n",
    "# Calculate gradient accumulation steps\n",
    "gradient_accumulation_steps = train_config.batch_size_training // train_config.micro_batch_size\n",
    "    \n",
    "# Load the pre-trained model and setup its configuration\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    train_config.model_name,\n",
    "    load_in_8bit=True if train_config.quantization else None,\n",
    "    device_map=\"auto\" if train_config.quantization else None,\n",
    ")\n",
    "\n",
    "print_model_size(model, train_config, rank if train_config.enable_fsdp else 0)\n",
    "\n",
    "# Prepare the model for int8 training if quantization is enabled\n",
    "if train_config.quantization:\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    \n",
    "# Convert the model to bfloat16 if fsdp and pure_bf16 is enabled\n",
    "if train_config.enable_fsdp and fsdp_config.pure_bf16:\n",
    "    model.to(torch.bfloat16)\n",
    "\n",
    "# Load the tokenizer and add special tokens\n",
    "tokenizer = LlamaTokenizer.from_pretrained(train_config.model_name, legacy=False)\n",
    "    # tokenizer.add_special_tokens(\n",
    "    #         {\n",
    "    #             \"pad_token\": \"<PAD>\",\n",
    "    #         }\n",
    "    #     )\n",
    "if train_config.use_peft:\n",
    "    # peft_config = generate_peft_config(train_config, kwags)\n",
    "    # model = get_peft_model(model, peft_config)\n",
    "    # model.print_trainable_parameters()\n",
    "    model = PeftModel.from_pretrained(model, 'peft_ckpt/ego4d_lta/lora/7B/0')\n",
    "    # pass\n",
    "#setting up FSDP if enable_fsdp is enabled\n",
    "if train_config.enable_fsdp:\n",
    "    if not train_config.use_peft and train_config.freeze_layers:\n",
    "        \n",
    "        freeze_transformer_layers(train_config.num_freeze_layers)\n",
    "\n",
    "    mixed_precision_policy, wrapping_policy = get_policies(fsdp_config, rank)\n",
    "    my_auto_wrapping_policy = fsdp_auto_wrap_policy(model, LlamaDecoderLayer)\n",
    "\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy= my_auto_wrapping_policy if train_config.use_peft else wrapping_policy,\n",
    "        mixed_precision=mixed_precision_policy if not fsdp_config.pure_bf16 else None,\n",
    "        sharding_strategy=fsdp_config.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        limit_all_gathers=True,\n",
    "    )\n",
    "    if fsdp_config.fsdp_activation_checkpointing:\n",
    "        policies.apply_fsdp_checkpointing(model)\n",
    "elif not train_config.quantization and not train_config.enable_fsdp:\n",
    "    model.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_config = generate_dataset_config(train_config, kwags)\n",
    "\n",
    "    # Load and preprocess the dataset for training and validation\n",
    "dataset_train = get_preprocessed_dataset(\n",
    "    tokenizer,\n",
    "    dataset_config,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "if not train_config.enable_fsdp or rank == 0:\n",
    "    print(f\"--> Training Set Length = {len(dataset_train)}\")\n",
    "\n",
    "dataset_val = get_preprocessed_dataset(\n",
    "    tokenizer,\n",
    "    dataset_config,\n",
    "    split=\"test\",\n",
    ")\n",
    "if not train_config.enable_fsdp or rank == 0:\n",
    "        print(f\"--> Validation Set Length = {len(dataset_val)}\")\n",
    "\n",
    "train_sampler = None\n",
    "val_sampler = None\n",
    "if train_config.enable_fsdp:\n",
    "    train_sampler = DistributedSampler(\n",
    "        dataset_train,\n",
    "        rank=dist.get_rank(),\n",
    "        num_replicas=dist.get_world_size(),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    if train_config.run_validation:\n",
    "        val_sampler = DistributedSampler(\n",
    "            dataset_val,\n",
    "            rank=dist.get_rank(),\n",
    "            num_replicas=dist.get_world_size(),\n",
    "        )\n",
    "    \n",
    "# Create DataLoaders for the training and validation dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=train_config.batch_size_training,\n",
    "    num_workers=train_config.num_workers_dataloader,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler if train_sampler else None,\n",
    "    drop_last=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "if train_config.run_validation:\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=train_config.val_batch_size,\n",
    "        num_workers=train_config.num_workers_dataloader,\n",
    "        pin_memory=True,\n",
    "        sampler=val_sampler if val_sampler else None,\n",
    "        drop_last=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    \n",
    "# Initialize the optimizer and learning rate scheduler\n",
    "if fsdp_config.pure_bf16 and fsdp_config.optimizer == \"anyprecision\":\n",
    "    optimizer = AnyPrecisionAdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config.lr,\n",
    "        momentum_dtype=torch.bfloat16,\n",
    "        variance_dtype=torch.bfloat16,\n",
    "        use_kahan_summation=False,\n",
    "    )\n",
    "else:\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config.lr,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.memory_utils import MemoryTrace\n",
    "\n",
    "def myevaluation(model,train_config, eval_dataloader, local_rank, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataloader\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        eval_dataloader: The dataloader containing the evaluation data\n",
    "        local_rank: The rank of the current node in a distributed setting\n",
    "        tokenizer: The tokenizer used to decode predictions\n",
    "    \n",
    "    Returns: eval_ppl, eval_epoch_loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    eval_preds = []\n",
    "    eval_loss = 0.0  # Initialize evaluation loss\n",
    "    eval_dataset_len = 0\n",
    "    with MemoryTrace() as memtrace:\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader,colour=\"green\", desc=\"evaluating Epoch\")):\n",
    "            # print(batch)\n",
    "            input_txt = tokenizer.decode(batch[\"input_ids\"][0].cpu().numpy(), skip_special_tokens=True)\n",
    "            # print(input_txt)\n",
    "            for key in batch.keys():\n",
    "                if train_config.enable_fsdp:\n",
    "                    batch[key] = batch[key].to(local_rank)\n",
    "                else:\n",
    "                    batch[key] = batch[key].to('cuda:0')\n",
    "            # Ensure no gradients are computed for this scope to save memory\n",
    "            with torch.no_grad():\n",
    "                # Forward pass and compute loss\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                eval_loss += loss.detach().float()\n",
    "                first_key = next(iter(batch))\n",
    "                eval_dataset_len+= len(batch[first_key])\n",
    "                \n",
    "            # Decode predictions and add to evaluation predictions list\n",
    "            preds = torch.argmax(outputs.logits, -1)\n",
    "            # print(\"***********************\")\n",
    "            print(tokenizer.batch_decode(preds.detach().cpu().numpy(), skip_special_tokens=True)[0])\n",
    "            print(\"-------------------------\")\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(preds.detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            )\n",
    "    \n",
    "    # If there's more than one CUDA device, reduce evaluation loss across all devices\n",
    "    if torch.cuda.device_count() > 1 and train_config.enable_fsdp:\n",
    "        dist.all_reduce(eval_loss, op=dist.ReduceOp.SUM)\n",
    "    \n",
    "    # Compute average loss and perplexity\n",
    "    eval_epoch_loss = eval_loss / eval_dataset_len\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\" {eval_ppl=} {eval_epoch_loss=}\")\n",
    "    return eval_ppl, eval_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval before training\n",
    "eval_results = myevaluation(\n",
    "    model, \n",
    "    train_config, \n",
    "    eval_dataloader, \n",
    "    local_rank if train_config.enable_fsdp else None, \n",
    "    tokenizer\n",
    ")\n",
    "print(f\"{eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/users/swang299/code/AntGPT-Llama2/llama-recipes\"\n",
    "import fire\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from inference.safety_utils import get_safety_checker\n",
    "from inference.model_utils import load_model, load_peft_model\n",
    "\n",
    "model_name = '/gpfs/data/superlab/models/llama2/llama/checkpoints/hf/Llama-2-7b-hf'\n",
    "peft_model = 'ego4d_lta/7B'\n",
    "quantization = True\n",
    "max_new_tokens =100 #The maximum numbers of tokens to generate\n",
    "seed=42 #seed value for reproducibility\n",
    "do_sample=True #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "min_length=None #The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "use_cache=True  #[optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "top_p=1.0 # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "temperature=1.0 # [optional] The value used to modulate the next token probabilities.\n",
    "top_k=50 # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "repetition_penalty=1.0 #The parameter for repetition penalty. 1.0 means no penalty.\n",
    "length_penalty=1, #[optional] Exponential penalty to the length that is used with beam-based generation. \n",
    "enable_azure_content_safety=False, # Enable safety check with Azure content safety api\n",
    "enable_sensitive_topics=False, # Enable check for sensitive topics using AuditNLG APIs\n",
    "enable_saleforce_content_safety=False, # Enable safety check woth Saleforce safety flan t5\n",
    "\n",
    "kwargs = {}\n",
    "# Set the seeds for reproducibility\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\"\"\"\n",
    "model = load_model(model_name, quantization)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "if peft_model:\n",
    "    model = load_peft_model(model, peft_model)\n",
    "\n",
    "model.eval()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Predict the next most possible 20 actions in the format of verb noun pair in chronological order that match the given observed 8 actions and common sense most. Below is the observed 8 actions.\\n\\n### Observed actions: attach pump, hold phone, hold phone, hold screw, hold screwdriver, put screwdriver, hold drill, put drill\\n\\n### Prediction: \"\n",
    "\n",
    "batch = tokenizer(user_prompt, return_tensors=\"pt\")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    # outputs = model.generate(**batch, max_new_tokens=100)\n",
    "    outputs = model.generate(\n",
    "        **batch,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        min_length=min_length,\n",
    "        use_cache=use_cache,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        **kwargs \n",
    "    )\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Model output:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Predict the next most possible 20 actions in the format of verb noun pair in chronological order that match the given observed 8 actions and common sense most. Below is the observed 8 actions.\\n\\n### Observed actions: take shirt, adjust shirt, take shirt, take shirt, take shirt, adjust shirt, adjust shirt, take iron\\n\\n### Prediction: \"\n",
    "user_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Describe a time when you had to make a difficult decision.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "batch = tokenizer(user_prompt, return_tensors=\"pt\")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "print(batch['input_ids'].shape)\n",
    "with torch.no_grad():\n",
    "    # Forward pass and compute loss\n",
    "    outputs = model(**batch)    \n",
    "# Decode predictions and add to evaluation predictions list\n",
    "print(outputs.logits.shape)\n",
    "preds = torch.argmax(outputs.logits, -1)\n",
    "print(tokenizer.batch_decode(preds.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
