{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oscar/data/csun45/swang299/code/AntGPT-Llama2/Finetune/llama-recipes\n"
     ]
    }
   ],
   "source": [
    "%cd \"/users/swang299/code/AntGPT-Llama2/Finetune/llama-recipes\"\n",
    "from transformers import LlamaModel, LlamaForCausalLM, LlamaConfig, LlamaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.modeling_llama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/users/swang299/code/AntGPT-Llama2/Finetune/llama-recipes/distill.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bccv-vscode-node/users/swang299/code/AntGPT-Llama2/Finetune/llama-recipes/distill.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bccv-vscode-node/users/swang299/code/AntGPT-Llama2/Finetune/llama-recipes/distill.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bccv-vscode-node/users/swang299/code/AntGPT-Llama2/Finetune/llama-recipes/distill.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_llama\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaForCausalLM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.modeling_llama'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
    "\n",
    "from utils.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisLlama(torch.nn.Module):\n",
    "    def __init__(self, llama_config, vis_size, use_vis=True):\n",
    "        super().__init__()\n",
    "        self.llama = LlamaForCausalLM(llama_config)\n",
    "        self.proj = torch.nn.Linear(vis_size, llama_config.hidden_size, bias=False)\n",
    "        if use_vis:\n",
    "            self.forward = self.forward_vis\n",
    "            self.generate = self.generate_vis\n",
    "        else:\n",
    "            self.forward = self.forward_no_vis\n",
    "            self.generate = self.generate_no_vis\n",
    "\n",
    "    def forward_no_vis(self, obs_feats, input_ids, labels, attention_mask):\n",
    "        inputs_embeds = self.llama.model.embed_tokens(input_ids)\n",
    "        batch = {\n",
    "            \"inputs_embeds\": inputs_embeds,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        outputs = self.llama(**batch)\n",
    "        return outputs\n",
    "    \n",
    "    def forward_vis(self, obs_feats, input_ids, labels, attention_mask):\n",
    "        bs = obs_feats.shape[0]\n",
    "        obs_embs = self.proj(obs_feats)\n",
    "        inputs_embeds = []\n",
    "        for i in range(bs):        \n",
    "            obs_emb = obs_embs[i]\n",
    "            input_id = input_ids[i]\n",
    "            # start of dummy input is the first -1 in input_ids\n",
    "            dummy_start = torch.where(input_id == -1)[0][0]\n",
    "            # end of dummy input is the last -1 in input_ids\n",
    "            dummy_end = torch.where(input_id == -1)[0][-1]\n",
    "            input_start = input_id[:dummy_start]\n",
    "            input_end = input_id[dummy_end+1:]\n",
    "            emb_start = self.llama.model.embed_tokens(input_start)\n",
    "            emb_end = self.llama.model.embed_tokens(input_end)\n",
    "            inputs_embed = torch.cat([emb_start, obs_emb, emb_end], dim=0)\n",
    "            inputs_embeds.append(inputs_embed)\n",
    "        inputs_embeds = torch.stack(inputs_embeds, dim=0)\n",
    "        batch = {\n",
    "            \"inputs_embeds\": inputs_embeds,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        outputs = self.llama(**batch)\n",
    "        return outputs\n",
    "\n",
    "    def generate_no_vis(self, obs_feats, input_ids, labels, attention_mask, max_new_tokens=200):\n",
    "        inputs_embeds = self.llama.model.embed_tokens(input_ids)\n",
    "        batch = {\n",
    "            \"inputs_embeds\": inputs_embeds,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llama.generate(\n",
    "                **batch,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                top_p=1.0,\n",
    "                temperature=0.3,\n",
    "                min_length=None,\n",
    "                use_cache=True,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.0,\n",
    "                length_penalty=1,\n",
    "                num_return_sequences = 5,\n",
    "                pad_token_id = 0,\n",
    "            )\n",
    "        return outputs\n",
    "    \n",
    "    def generate_vis(self, obs_feats, input_ids, labels, attention_mask, max_new_tokens=200):\n",
    "        bs = obs_feats.shape[0]\n",
    "        obs_emb = self.proj(obs_feats[0])\n",
    "        input_id = input_ids[0]\n",
    "        \n",
    "        label_start = torch.where(labels[0] != -100)[0][0]\n",
    "        input_id = input_id[:label_start]\n",
    "        \n",
    "        # start of dummy input is the first -1 in input_ids\n",
    "        dummy_start = torch.where(input_id == -1)[0][0]\n",
    "        # end of dummy input is the last -1 in input_ids\n",
    "        dummy_end = torch.where(input_id == -1)[0][-1]\n",
    "        input_start = input_id[:dummy_start]\n",
    "        input_end = input_id[dummy_end+1:]\n",
    "        emb_start = self.llama.model.embed_tokens(input_start)\n",
    "        emb_end = self.llama.model.embed_tokens(input_end)\n",
    "        inputs_embed = torch.cat([emb_start, obs_emb, emb_end], dim=0)\n",
    "        batch = {\n",
    "            \"inputs_embeds\": inputs_embed.unsqueeze(0),\n",
    "            \"attention_mask\": attention_mask[0][:inputs_embed.shape[0]].unsqueeze(0),\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llama.generate(\n",
    "                **batch,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                top_p=1.0,\n",
    "                temperature=0.3,\n",
    "                min_length=None,\n",
    "                use_cache=True,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.0,\n",
    "                length_penalty=1,\n",
    "                num_return_sequences = 5,\n",
    "                pad_token_id = 0,\n",
    "            )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisLlama(\n",
       "  (llama): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       "  )\n",
       "  (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_config = LlamaConfig(vocab_size = 32000,\n",
    "                    hidden_size = 768,\n",
    "                    intermediate_size = 2048,\n",
    "                    num_hidden_layers = 6,\n",
    "                    num_attention_heads = 6,\n",
    "                    num_key_value_heads = 6,\n",
    "                    max_position_embeddings = 300,\n",
    "                    )\n",
    "model = VisLlama(llama_config, vis_size=768, use_vis=True)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "# detect model's name is visllama\n",
    "if model.__class__.__name__ == \"VisLlama\":\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a LLaMA style configuration\n",
    "\n",
    "configuration = LlamaConfig(vocab_size = 32000,\n",
    "                            hidden_size = 768,\n",
    "                            intermediate_size = 2048,\n",
    "                            num_hidden_layers = 6,\n",
    "                            num_attention_heads = 6,\n",
    "                            num_key_value_heads = 6,\n",
    "                            max_position_embeddings = 300,\n",
    "                            )\n",
    "\n",
    "# Initializing a model from the llama-7b style configuration\n",
    "model = LlamaForCausalLM(configuration)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlamaTokenizer = LlamaTokenizer.from_pretrained('/gpfs/data/superlab/models/llama2/llama/checkpoints/hf/Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.decode(LlamaTokenizer.encode(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 263, 29892, 29890]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.encode(\"a,b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a,b'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.decode([263, 29892, 29890])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "model.load_state_dict(torch.load('ft_ckpt/ego4d_v1/layer6_bs64_15e-4/11.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the model except for embed_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
